<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[视觉]]></title><description><![CDATA[Obsidian digital garden]]></description><link>https://www.glacierchen.asia/</link><image><url>https://www.glacierchen.asia/lib/media/favicon.png</url><title>视觉</title><link>https://www.glacierchen.asia/</link></image><generator>Webpage HTML Export plugin for Obsidian</generator><lastBuildDate>Tue, 01 Oct 2024 14:27:23 GMT</lastBuildDate><atom:link href="https://www.glacierchen.asia/lib/rss.xml" rel="self" type="application/rss+xml"/><pubDate>Tue, 01 Oct 2024 14:27:22 GMT</pubDate><copyright><![CDATA[Glacier]]></copyright><ttl>60</ttl><dc:creator>Glacier</dc:creator><item><title><![CDATA[论文：Image amodal completion _A survey]]></title><description><![CDATA[ 
 <br><br>occlusion handling problem<br><br><br>非模态形状补全是指利用部分视觉证据推测物体的完整形状，包括被遮挡部分的形状推测。这一任务的目标是恢复物体的整体轮廓，从而更好地理解场景中的物体大小和相对深度。为了实现这一目标，主要采用以下几种方法：<br>
<br>Amodal Instance Segmentation（非模态实例分割）：

<br>这是最常用的非模态形状补全技术，其目标是预测一个实例的完整分割掩码（包括可见和不可见部分）。
<br>例如，Mask R-CNN 模型经过扩展后可以实现非模态形状补全，而不是仅仅关注可见部分的分割。
<br>根据是否结合目标检测技术，可以将这些方法分为两大类：

<br>结合目标检测的非模态实例分割：

<br>首先使用目标检测方法获取目标所在区域（如边界框），然后在候选区域内进行像素级分割，生成物体的分割掩码。
<br>例如，Li 和 Malik（2016）提出的 IBBE 方法，通过迭代地调整边界框，逐步推测物体的完整形状。


<br>不结合目标检测的非模态实例分割：

<br>直接在像素级别进行形状补全，而不依赖于目标检测框。
<br>例如，基于深度学习的 AmodalMask（Zhu et al., 2017）模型通过输入图像块预测被遮挡物体的非模态掩码。






<br>几何形状补全方法：

<br>一些早期的研究尝试使用几何模型（如 Euler 螺旋、直线和贝塞尔曲线）来推测被遮挡部分的形状。
<br>这些方法通常只适用于简单形状（如直线或圆形），在复杂场景中效果较差。


<br><br>非模态外观补全的目标是恢复被遮挡部分的物体表面外观（即 RGB 值），使得物体看起来完整且一致。大多数当前的非模态外观补全方法都依赖于从非模态形状补全中获得的形状掩码，分为以下几种类型：<br>
<br>物体中心表示（Object-centric Representations for Toy Datasets）：

<br>主要用于简单的玩具数据集，例如 MONet 和 IODINE，使用变分自编码器（VAE）结合注意力机制来重建物体的外观。
<br>这些方法主要在颜色对比度明显的简单背景下有效，难以应用于复杂场景。


<br>特定类别的外观补全（Category-specific Studies）：

<br>针对某些特定类型的物体（如车辆或人体）进行非模态外观补全。
<br>例如，Yan 等人（2019）提出了一个 GAN-based 的框架，用于恢复被遮挡车辆的外观，而 Zhou 等人（2021）提出了一种用于人体外观补全的 Unet 模型。


<br>复杂场景的外观补全（Methods for Complex Scenes）：

<br>针对真实场景中较复杂的物体或背景进行外观补全。
<br>这些方法通常需要使用合成数据进行训练，如 SeGAN（Ehsani et al., 2018）使用 cGAN 来生成物体的被遮挡部分的外观。


<br><br>顺序感知是指理解物体在场景中的相互遮挡关系（Occlusion Order）或深度关系（Layer Order），并依此推测物体的排列顺序。顺序感知可以帮助提升形状和外观补全的精度：<br>
<br>Occlusion Order（遮挡顺序）：

<br>通过推测物体间的遮挡关系（如物体 A 遮挡物体 B），来辅助形状和外观补全任务。
<br>例如，BCNet（Ke et al., 2021）使用图卷积网络（GCN）将重叠区域划分为不同的遮挡层，从而分离遮挡物和被遮挡物。


<br>Layer Order（图层顺序）：

<br>图层顺序主要用于确定物体的深度关系（前景和背景的关系）。
<br>例如，一些研究将场景划分为前景和背景两个层（Dhamo et al., 2019），或使用多图层结构表示更复杂的物体深度关系（Zheng et al., 2021）。


<br><br><br><br>手动标注是指通过人工方式对图像进行标注，即由人类标注者根据视觉感知能力来预测物体的被遮挡部分，并手动标注这些不可见区域的轮廓或外观。这种方法是图像非模态补全领域中最直接、最传统的标注方式，具有较高的准确性和一致性，但其缺点在于标注成本高且耗时耗力。<br>
<br>具体描述：

<br>标注者通常需要通过想象物体的被遮挡部分来标记物体的完整轮廓（例如，将物体看作完全可见的形式进行标注），从而生成非模态分割掩码（amodal segmentation mask）或相应的边界框。
<br>标注者之间的意见可能会有差异，但 Zhu et al. (2017) 的研究表明，人类标注者在预测被遮挡区域时通常具有较高的一致性。
<br>这一方法通常用于生成非模态形状补全（amodal shape completion）和遮挡顺序（occlusion order）的标注数据集。


<br>优点：

<br>由于是由人类标注者依据真实的视觉经验进行标注，标注结果往往具有较高的质量和一致性。
<br>可以提供复杂场景中不同物体类别的完整形状标注，从而为模型提供精确的非模态分割掩码。


<br>局限性：

<br>标注成本高：手动标注数据集需要投入大量的人工资源和时间。
<br>标注结果通常仅限于物体的非模态形状（即掩码或轮廓），无法提供被遮挡区域的 RGB 外观信息。


<br>代表性数据集：

<br>COCOA（Zhu et al., 2017）数据集：手动标注了 COCO 数据集中部分图像的非模态分割掩码和物体间的遮挡顺序。
<br>KINS（Qi et al., 2019）数据集：在 KITTI 数据集上进行了车辆和行人的手动标注，生成了非模态分割掩码和相对遮挡顺序信息。


<br><br>基于剪贴画的图像合成是一种较为低成本的方式，通过将现有图像中剪切出的物体片段（patches）叠加到目标图像上，从而模拟被遮挡的场景。这种方法可以为模型提供大规模的非模态外观数据，但其生成的图像通常不够真实，并且容易产生伪影（artifacts）。<br>
<br>具体描述：

<br>研究人员首先将某些目标物体（例如汽车、人物）的图像块从背景中剪切出来，然后将这些图像块叠加到另一幅目标图像中，形成遮挡关系。
<br>通过剪贴画合成的方式，可以自动生成物体的非模态分割掩码和完整外观（RGB appearance）。
<br>由于物体是从原图中剪切出来的，因此可以在合成图像中准确地提供被遮挡部分的外观数据。


<br>优点：

<br>标注成本低：相比手动标注，通过合成方式可以快速生成大规模的非模态数据集。
<br>提供了被遮挡区域的完整外观标注，而不仅仅是物体轮廓，从而适用于非模态外观补全任务。


<br>局限性：

<br>只能处理通过剪贴画叠加产生的遮挡，难以处理复杂背景中的多物体遮挡。
<br>合成图像可能会因为拼接过程而出现不自然的边界或纹理伪影，影响训练效果。
<br>难以模拟复杂真实场景中的光照变化和深度关系，合成图像的逼真度和多样性不足。


<br>代表性数据集：

<br>D2S Amodal（Follmann et al., 2019）：通过叠加超市商品的图像片段生成了大量具有非模态分割掩码的合成图像。
<br>OVD（Yan et al., 2019）：通过将 COCO 数据集中的物体剪贴到车辆图像上，生成了被遮挡车辆的合成图像，并提供了车辆的完整外观标注。


<br><br>这种方式通过构建 3D 场景，然后生成相应的 2D 图像，从而获得完整的物体标注。合成的 3D 场景能够精确地控制物体的遮挡关系、深度信息和 RGB 外观，因此可以生成高质量的标注数据集，并且能够提供物体间的复杂交互信息。<br>
<br>具体描述：

<br>首先在 3D 环境中构建一个包含多个物体的场景（如模拟街景或室内场景），然后通过改变视角来生成不同的 2D 图像。
<br>这些合成图像可以提供物体的非模态分割掩码、完整的 RGB 外观、深度信息（Depth Map）以及遮挡关系（Occlusion Order）。
<br>由于这些图像是从 3D 模型生成的，因此可以轻松获取物体的完整形状和外观，而不受遮挡的影响。


<br>优点：

<br>标注结果高度精确且具有多样性：合成场景中可以任意控制物体的姿态、位置、遮挡关系和光照条件，从而生成具有高度准确性的标注数据。
<br>提供丰富的场景信息：如深度图、遮挡顺序、物体间的空间关系等，可以辅助非模态形状和外观补全任务。


<br>局限性：

<br>合成场景的构建通常需要复杂的3D建模技术和渲染过程，生成数据集的成本较高。
<br>合成图像的视觉效果和细节可能与真实场景存在较大差距，导致模型在真实场景中的泛化能力不足。


<br>代表性数据集：

<br>SAIL-VOS（Hu et al., 2019）：使用 GTA-V 环境模拟生成了大量室内和室外场景的图像，并对其中的物体进行了完整标注。
<br>CSD（Zheng et al., 2021）：在合成的室内场景中生成了具有多遮挡关系的图像，提供了丰富的非模态形状和外观标注。


<br><br>这一部分系统性地总结了图像非模态补全任务中常用的数据集生成方式，并详细分析了每种方式的优点和局限性。对于不同的研究任务，可以根据任务需求选择合适的数据集构建方法。<br>
<br>手动标注 适用于需要高质量、精确形状标注的任务，但成本高昂。
<br>基于剪贴画的图像合成 适用于大规模数据集的生成，但生成图像的质量受限于拼接过程。
<br>基于 3D 场景的合成图像 能够提供高质量的标注数据，但在视觉效果上可能与真实场景存在差距。
<br><br><br>该部分将现有数据集分为三大类，分别是：<br>
<br>Common Objects Datasets（通用物体数据集）
<br>Vehicle Datasets（车辆数据集）
<br>Specialized Datasets（专用数据集：人体、室内场景等）
]]></description><link>https://www.glacierchen.asia/非模态分割/论文：Image amodal completion _A survey.html</link><guid isPermaLink="false">非模态分割/论文：Image amodal completion _A survey.md</guid><dc:creator><![CDATA[Glacier]]></dc:creator><pubDate>Tue, 01 Oct 2024 06:20:55 GMT</pubDate></item></channel></rss>